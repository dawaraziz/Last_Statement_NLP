{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle\n",
      "\u001b[31m  Could not find a version that satisfies the requirement pickle (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for pickle\u001b[0m\n",
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in /anaconda3/lib/python3.6/site-packages (from sklearn) (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /anaconda3/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /anaconda3/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Running setup.py bdist_wheel for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/lucmeng/Library/Caches/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "Requirement already satisfied: matplotlib in /anaconda3/lib/python3.6/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /anaconda3/lib/python3.6/site-packages (from matplotlib) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /anaconda3/lib/python3.6/site-packages (from matplotlib) (1.15.4)\n",
      "Requirement already satisfied: six>=1.10 in /anaconda3/lib/python3.6/site-packages (from matplotlib) (1.11.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /anaconda3/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /anaconda3/lib/python3.6/site-packages (from matplotlib) (2.2.0)\n",
      "Requirement already satisfied: pytz in /anaconda3/lib/python3.6/site-packages (from matplotlib) (2018.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /anaconda3/lib/python3.6/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0)\n",
      "Requirement already satisfied: fastText in /anaconda3/lib/python3.6/site-packages (0.8.22)\n",
      "Requirement already satisfied: pybind11>=2.2 in /anaconda3/lib/python3.6/site-packages (from fastText) (2.2.4)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /anaconda3/lib/python3.6/site-packages (from fastText) (39.1.0)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (from fastText) (1.15.4)\n",
      "Requirement already satisfied: spacy in /anaconda3/lib/python3.6/site-packages (2.0.16)\n",
      "Requirement already satisfied: ujson>=1.35 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.15.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.4.3.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (6.12.0)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: regex==2018.01.10 in /anaconda3/lib/python3.6/site-packages (from spacy) (2018.1.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.18.4)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: msgpack>=0.3.0 in /anaconda3/lib/python3.6/site-packages (from msgpack-numpy<0.4.4->spacy) (0.5.6)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.0->spacy) (1.11.0)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.0->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.0->spacy) (4.26.0)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.0->spacy) (1.10.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.10.15)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /anaconda3/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.0->spacy) (0.9.0)\n",
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied: tensorflow in /anaconda3/lib/python3.6/site-packages (1.11.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: setuptools<=39.1.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (39.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.15.4)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.5 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.0.6)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.5.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.3 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.0.5)\n",
      "Requirement already satisfied: tensorboard<1.12.0,>=1.11.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.5->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /anaconda3/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow) (2.6.11)\n",
      "Requirement already satisfied: tensorflow_hub in /anaconda3/lib/python3.6/site-packages (0.1.1)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow_hub) (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow_hub) (1.15.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow_hub) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow_hub) (39.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "!pip install scrapy\n",
    "\n",
    "!pip install fastText\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import fastText\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.selector import Selector\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Show Python version\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Offenders Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffenderInfoWriterPipeline(object):\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('data/offender_info_results.json', 'w+')\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffenderInfoSpider(scrapy.Spider):\n",
    "    name = \"OffenderInfo\"\n",
    "    start_urls = [\n",
    "        'http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html'\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.OffenderInfoWriterPipeline': 1}\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        print('A response from %s just arrived!', response.url)\n",
    "        sel = Selector(response)\n",
    "\n",
    "        table = sel.xpath('//table[@class=\"tdcj_table indent\"]/tr')\n",
    "        for tr in table[1:]:\n",
    "            url_info = urljoin(response.url, str(tr.xpath('td[2]/a/@href').extract_first()))\n",
    "            url_stmt = urljoin(response.url, str(tr.xpath('td[3]/a/@href').extract_first()))\n",
    "        \n",
    "            yield {\n",
    "                'first_name': tr.xpath('td[5]/text()').extract_first(),\n",
    "                'last_name': tr.xpath('td[4]/text()').extract_first(),\n",
    "                'age': tr.xpath('td[7]/text()').extract_first(),\n",
    "                'date': tr.xpath('td[8]/text()').extract_first(),\n",
    "                'race': tr.xpath('td[9]/text()').extract_first(),\n",
    "                'country': tr.xpath('td[10]/text()').extract_first(),\n",
    "                'info_link': url_info,\n",
    "                'death_note_link': url_stmt\n",
    "            }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 17:02:51 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)\n",
      "2018-11-25 17:02:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2p  14 Aug 2018), cryptography 2.3.1, Platform Darwin-17.7.0-x86_64-i386-64bit\n",
      "2018-11-25 17:02:51 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x1a2e33cfd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A response from %s just arrived! http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess()\n",
    "process.crawl(OffenderInfoSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first_name',\n",
      " 'last_name',\n",
      " 'age',\n",
      " 'date',\n",
      " 'race',\n",
      " 'country',\n",
      " 'info_link',\n",
      " 'death_note_link']\n"
     ]
    }
   ],
   "source": [
    "file = open('data/offender_info_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "line = lines[0]\n",
    "obj = json.loads(line)\n",
    "pprint([key for key in obj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Death Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "objs = []\n",
    "\n",
    "file = open('data/offender_info_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "for line in lines:\n",
    "    objs.append(json.loads(line))\n",
    "    urls.append(objs[-1]['death_note_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeathNoteWriterPipeline(object):\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('data/death_note_results.json', 'w+')\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "    def process_it### Define the spiderem(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "class DeathNoteSpider(scrapy.Spider):\n",
    "    name = \"DeathNote\"\n",
    "    start_urls = urls\n",
    "    \n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.DeathNoteWriterPipeline': 1}\n",
    "    }\n",
    "\n",
    "    def parse(self, response):     \n",
    "        sel = Selector(response)\n",
    "    \n",
    "        first = str(sel.xpath('//div[@id=\"content_right\"]/p[6]/text()').extract_first()).strip()\n",
    "        second = str(sel.xpath('//div[@id=\"content_right\"]/p[7]/text()').extract_first()).strip()\n",
    "        \n",
    "        death_note = ''\n",
    "        if first and first != 'Last Statement:' and first != 'None':\n",
    "            death_note += first\n",
    "        if second and second != 'None':\n",
    "            death_note += second\n",
    "            \n",
    "        url = response.url\n",
    "        obj = [o for o in objs if o['death_note_link'] == url][0]\n",
    "        obj['death_note'] = death_note\n",
    "    \n",
    "        yield obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 17:03:21 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)\n",
      "2018-11-25 17:03:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2p  14 Aug 2018), cryptography 2.3.1, Platform Darwin-17.7.0-x86_64-i386-64bit\n",
      "2018-11-25 17:03:21 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 30, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x1a2e821320>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "process.crawl(DeathNoteSpider())\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/offender_info_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "objs = []\n",
    "for line in lines:\n",
    "    objs.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Format Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(objs)):\n",
    "    note = objs[i]['death_note']\n",
    "    if len(note.strip()) > 0:\n",
    "        note = (note.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "        objs[i]['death_note'] = note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Punctuations and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) - set('not')\n",
    "rm_punc = re.compile('[^a-zA-Z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs_clean = []\n",
    "death_notes_clean = []\n",
    "for i in range(len(objs)):\n",
    "    note = objs[i]['death_note']\n",
    "    note = rm_punc.sub(' ', note)\n",
    "    words = []\n",
    "    tokens = note.split()\n",
    "    for token in tokens:\n",
    "        if token and token not in stop_words and token.lower() not in stop_words \\\n",
    "        and token.lower() != 'none':\n",
    "            words.append(token)\n",
    "    \n",
    "    if words:\n",
    "        note = ' '.join(words)\n",
    "        death_notes_clean.append(note)\n",
    "        obj[i]['death_note'] = note\n",
    "        objs_clean.append(obj[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/death_note_clean.pickle', 'wb') as handle:\n",
    "    pickle.dump(objs_clean, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(death_notes_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs_lemma = []\n",
    "death_notes_lemma = []\n",
    "for i in range(len(objs_clean)):\n",
    "    obj = objs_clean[i]\n",
    "    \n",
    "    note = clean(obj['death_note'])\n",
    "    death_notes_lemma.append(note)\n",
    "    obj['death_note'] = note\n",
    "    objs_lemma.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/death_notes_lemma.pickle', 'wb') as handle:\n",
    "    pickle.dump(objs_lemma, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = {}\n",
    "statement_word_count = {}\n",
    "for s_id in range(len(last_words)):\n",
    "    words_list = last_words.iloc[s_id]['LastStatement'].split()\n",
    "    statement_word_count[s_id] = len(words_list)\n",
    "    bag_of_words[s_id] = {}\n",
    "    for word in words_list:\n",
    "        if word in bag_of_words[s_id]:\n",
    "            bag_of_words[s_id][word] = bag_of_words[s_id][word] +1\n",
    "        else:\n",
    "            bag_of_words[s_id][word] = 1\n",
    "\n",
    "for s_id in bag_of_words:\n",
    "    sorted_x = dict(sorted(bag_of_words[s_id].items(), key=lambda kv: kv[1], reverse=True))\n",
    "    bag_of_words[s_id] = sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_notes = death_notes_lemma\n",
    "# death_notes = death_notes_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get notes in each clsuter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_clusters = {}\n",
    "for c in set(clusters):\n",
    "    note_clusters[c] = []\n",
    "    for i, txt in enumerate(death_notes):\n",
    "        if clusters[i] == c:\n",
    "            note_clusters[c].append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top K Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
