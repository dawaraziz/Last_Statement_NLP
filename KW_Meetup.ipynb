{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in /anaconda3/lib/python3.6/site-packages (from sklearn) (0.19.1)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Running setup.py bdist_wheel for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/biksingh/Library/Caches/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "Requirement already satisfied: matplotlib in /anaconda3/lib/python3.6/site-packages (2.2.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /anaconda3/lib/python3.6/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /anaconda3/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pytz in /anaconda3/lib/python3.6/site-packages (from matplotlib) (2017.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /anaconda3/lib/python3.6/site-packages (from matplotlib) (2.7.3)\n",
      "Requirement already satisfied: six>=1.10 in /anaconda3/lib/python3.6/site-packages (from matplotlib) (1.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /anaconda3/lib/python3.6/site-packages (from matplotlib) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /anaconda3/lib/python3.6/site-packages (from matplotlib) (1.15.2)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0)\n",
      "Collecting scrapy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/12/a6197eaf97385e96fd8ec56627749a6229a9b3178ad73866a0b1fb377379/Scrapy-1.5.1-py2.py3-none-any.whl (249kB)\n",
      "\u001b[K    100% |████████████████████████████████| 256kB 5.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
      "Requirement already satisfied: pyOpenSSL in /anaconda3/lib/python3.6/site-packages (from scrapy) (18.0.0)\n",
      "Collecting parsel>=1.1 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/96/69/d1d5dba5e4fecd41ffd71345863ed36a45975812c06ba77798fc15db6a64/parsel-1.5.1-py2.py3-none-any.whl\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/37/94/40c93ad0cadac0f8cb729e1668823c71532fd4a7361b141aec535acb68e3/w3lib-1.19.0-py2.py3-none-any.whl\n",
      "Collecting service-identity (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/29/fa/995e364220979e577e7ca232440961db0bf996b6edaf586a7d1bd14d81f1/service_identity-17.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Twisted>=13.1.0 in /anaconda3/lib/python3.6/site-packages (from scrapy) (18.7.0)\n",
      "Collecting queuelib (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5.2 in /anaconda3/lib/python3.6/site-packages (from scrapy) (1.10.0)\n",
      "Collecting cssselect>=0.9 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: lxml in /anaconda3/lib/python3.6/site-packages (from scrapy) (4.2.1)\n",
      "Requirement already satisfied: cryptography>=2.2.1 in /anaconda3/lib/python3.6/site-packages (from pyOpenSSL->scrapy) (2.2.2)\n",
      "Requirement already satisfied: pyasn1-modules in /anaconda3/lib/python3.6/site-packages (from service-identity->scrapy) (0.2.2)\n",
      "Requirement already satisfied: pyasn1 in /anaconda3/lib/python3.6/site-packages (from service-identity->scrapy) (0.4.4)\n",
      "Requirement already satisfied: attrs in /anaconda3/lib/python3.6/site-packages (from service-identity->scrapy) (18.1.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /anaconda3/lib/python3.6/site-packages (from Twisted>=13.1.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in /anaconda3/lib/python3.6/site-packages (from Twisted>=13.1.0->scrapy) (17.5.0)\n",
      "Requirement already satisfied: PyHamcrest>=1.9.0 in /anaconda3/lib/python3.6/site-packages (from Twisted>=13.1.0->scrapy) (1.9.0)\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /anaconda3/lib/python3.6/site-packages (from Twisted>=13.1.0->scrapy) (4.5.0)\n",
      "Requirement already satisfied: Automat>=0.3.0 in /anaconda3/lib/python3.6/site-packages (from Twisted>=13.1.0->scrapy) (0.7.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /anaconda3/lib/python3.6/site-packages (from Twisted>=13.1.0->scrapy) (17.3.1)\n",
      "Requirement already satisfied: idna>=2.1 in /anaconda3/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy) (2.6)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /anaconda3/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy) (0.24.0)\n",
      "Requirement already satisfied: cffi>=1.7 in /anaconda3/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy) (1.11.2)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from PyHamcrest>=1.9.0->Twisted>=13.1.0->scrapy) (39.1.0)\n",
      "Requirement already satisfied: pycparser in /anaconda3/lib/python3.6/site-packages (from cffi>=1.7->cryptography>=2.2.1->pyOpenSSL->scrapy) (2.18)\n",
      "Building wheels for collected packages: PyDispatcher\n",
      "  Running setup.py bdist_wheel for PyDispatcher ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/biksingh/Library/Caches/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
      "Successfully built PyDispatcher\n",
      "Installing collected packages: PyDispatcher, w3lib, cssselect, parsel, service-identity, queuelib, scrapy\n",
      "Successfully installed PyDispatcher-2.0.5 cssselect-1.0.3 parsel-1.5.1 queuelib-1.5.0 scrapy-1.5.1 service-identity-17.0.0 w3lib-1.19.0\n",
      "Requirement already satisfied: fastText in /anaconda3/lib/python3.6/site-packages (0.8.22)\n",
      "Requirement already satisfied: pybind11>=2.2 in /anaconda3/lib/python3.6/site-packages (from fastText) (2.2.4)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /anaconda3/lib/python3.6/site-packages (from fastText) (39.1.0)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (from fastText) (1.15.2)\n",
      "Requirement already satisfied: spacy in /anaconda3/lib/python3.6/site-packages (2.0.16)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (6.12.0)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.18.4)\n",
      "Requirement already satisfied: ujson>=1.35 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: regex==2018.01.10 in /anaconda3/lib/python3.6/site-packages (from spacy) (2018.1.10)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.4.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.15.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.0->spacy) (4.19.5)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.0->spacy) (0.5.6)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.0->spacy) (1.10.11)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.0->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.0->spacy) (1.10.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.4.16)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /anaconda3/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.0->spacy) (0.9.0)\n",
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from nltk) (1.10.0)\n",
      "Requirement already satisfied: tensorflow in /anaconda3/lib/python3.6/site-packages (1.11.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.15.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.5 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.0.6)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.3 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.0.5)\n",
      "Requirement already satisfied: tensorboard<1.12.0,>=1.11.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.10.0)\n",
      "Requirement already satisfied: setuptools<=39.1.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (39.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.5->tensorflow) (2.7.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /anaconda3/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: tensorflow_hub in /anaconda3/lib/python3.6/site-packages (0.1.1)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow_hub) (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow_hub) (1.15.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow_hub) (1.10.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow_hub) (39.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "!pip install scrapy\n",
    "\n",
    "!pip install fastText\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import fastText\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.selector import Selector\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Show Python version\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Offenders Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffenderInfoWriterPipeline(object):\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('data/offender_info_results.json', 'w+')\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffenderInfoSpider(scrapy.Spider):\n",
    "    name = \"OffenderInfo\"\n",
    "    start_urls = [\n",
    "        'http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html'\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.OffenderInfoWriterPipeline': 1}\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        print('A response from %s just arrived!', response.url)\n",
    "        sel = Selector(response)\n",
    "\n",
    "        table = sel.xpath('//table[@class=\"tdcj_table indent\"]/tr')\n",
    "        for tr in table[1:]:\n",
    "            url_info = urljoin(response.url, str(tr.xpath('td[2]/a/@href').extract_first()))\n",
    "            url_stmt = urljoin(response.url, str(tr.xpath('td[3]/a/@href').extract_first()))\n",
    "        \n",
    "            yield {\n",
    "                'first_name': tr.xpath('td[5]/text()').extract_first(),\n",
    "                'last_name': tr.xpath('td[4]/text()').extract_first(),\n",
    "                'age': tr.xpath('td[7]/text()').extract_first(),\n",
    "                'date': tr.xpath('td[8]/text()').extract_first(),\n",
    "                'race': tr.xpath('td[9]/text()').extract_first(),\n",
    "                'country': tr.xpath('td[10]/text()').extract_first(),\n",
    "                'info_link': url_info,\n",
    "                'death_note_link': url_stmt\n",
    "            }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 18:03:29 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)\n",
      "2018-11-25 18:03:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2p  14 Aug 2018), cryptography 2.3.1, Platform Darwin-17.7.0-x86_64-i386-64bit\n",
      "2018-11-25 18:03:29 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x1a2c6ac160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process = CrawlerProcess()\n",
    "process.crawl(OffenderInfoSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/offender_info_results.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-781f803906c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/offender_info_results.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/offender_info_results.json'"
     ]
    }
   ],
   "source": [
    "file = open('data/offender_info_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "line = lines[0]\n",
    "obj = json.loads(line)\n",
    "pprint([key for key in obj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Death Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "objs = []\n",
    "\n",
    "file = open('data/offender_info_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "for line in lines:\n",
    "    objs.append(json.loads(line))\n",
    "    urls.append(objs[-1]['death_note_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeathNoteWriterPipeline(object):\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('data/death_note_results.json', 'w+')\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "class DeathNoteSpider(scrapy.Spider):\n",
    "    name = \"DeathNote\"\n",
    "    start_urls = urls\n",
    "    \n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.DeathNoteWriterPipeline': 1}\n",
    "    }\n",
    "\n",
    "    def parse(self, response):     \n",
    "        sel = Selector(response)\n",
    "    \n",
    "        first = str(sel.xpath('//div[@id=\"content_right\"]/p[6]/text()').extract_first()).strip()\n",
    "        second = str(sel.xpath('//div[@id=\"content_right\"]/p[7]/text()').extract_first()).strip()\n",
    "        \n",
    "        death_note = ''\n",
    "        if first and first != 'Last Statement:' and first != 'None':\n",
    "            death_note += first\n",
    "        if second and second != 'None':\n",
    "            death_note += second\n",
    "            \n",
    "        url = response.url\n",
    "        obj = [o for o in objs if o['death_note_link'] == url][0]\n",
    "        obj['death_note'] = death_note\n",
    "    \n",
    "        yield obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 18:05:33 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)\n",
      "2018-11-25 18:05:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2p  14 Aug 2018), cryptography 2.3.1, Platform Darwin-17.7.0-x86_64-i386-64bit\n",
      "2018-11-25 18:05:33 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 30, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x1a2fa19438>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "process.crawl(DeathNoteSpider())\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/death_note_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "objs = []\n",
    "for line in lines:\n",
    "    objs.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Format Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(objs)):\n",
    "    note = objs[i]['death_note']\n",
    "    if len(note.strip()) > 0:\n",
    "        note = (note.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "        objs[i]['death_note'] = note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Punctuations and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) - set('not')\n",
    "rm_punc = re.compile('[^a-zA-Z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs_clean = []\n",
    "death_notes_clean = []\n",
    "for i in range(len(objs)):\n",
    "    note = objs[i]['death_note']\n",
    "    note = rm_punc.sub(' ', note)\n",
    "    words = []\n",
    "    tokens = note.split()\n",
    "    for token in tokens:\n",
    "        if token and token not in stop_words and token.lower() not in stop_words \\\n",
    "        and token.lower() != 'none':\n",
    "            words.append(token)\n",
    "    \n",
    "    if words:\n",
    "        note = ' '.join(words)\n",
    "        death_notes_clean.append(note)\n",
    "        objs[i]['death_note'] = note\n",
    "        objs_clean.append(objs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/death_note_clean.pickle', 'wb') as handle:\n",
    "    pickle.dump(objs_clean, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(death_notes_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs_lemma = []\n",
    "death_notes_lemma = []\n",
    "for i in range(len(objs_clean)):\n",
    "    obj = objs_clean[i]\n",
    "    \n",
    "    note = clean(obj['death_note'])\n",
    "    death_notes_lemma.append(note)\n",
    "    obj['death_note'] = note\n",
    "    objs_lemma.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/death_notes_lemma.pickle', 'wb') as handle:\n",
    "    pickle.dump(objs_lemma, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per doc\n",
    "bag_of_words = {}\n",
    "statement_word_count = {}\n",
    "for s_id in range(len(last_words)):\n",
    "    words_list = last_words.iloc[s_id]['LastStatement'].split()\n",
    "    statement_word_count[s_id] = len(words_list)\n",
    "    bag_of_words[s_id] = {}\n",
    "    for word in words_list:\n",
    "        if word in bag_of_words[s_id]:\n",
    "            bag_of_words[s_id][word] = bag_of_words[s_id][word] +1\n",
    "        else:\n",
    "            bag_of_words[s_id][word] = 1\n",
    "\n",
    "for s_id in bag_of_words:\n",
    "    sorted_x = dict(sorted(bag_of_words[s_id].items(), key=lambda kv: kv[1], reverse=True))\n",
    "    bag_of_words[s_id] = sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all doc\n",
    "statementFreqDict = {}\n",
    "\n",
    "for s_id in bag_of_words:\n",
    "    for word in bag_of_words[s_id]:\n",
    "        if word in statementFreqDict:\n",
    "            statementFreqDict[word] = statementFreqDict[word] + bag_of_words[s_id][word]\n",
    "        else:\n",
    "            statementFreqDict[word] = bag_of_words[s_id][word]\n",
    "        \n",
    "sorted_x = dict(sorted(statementFreqDict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "statementFreqDict = sorted_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claculating TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(ngram, doc_id, ngram_count, ngramDocDict):\n",
    "    return ngramDocDict[doc_id][ngram] / ngram_count[doc_id]\n",
    "\n",
    "def n_containing(ngram, ngramDocDict):\n",
    "    return sum(1 for doc_id in ngramDocDict if ngram in ngramDocDict[doc_id])\n",
    "\n",
    "def idf(ngram, ngramDocDict):\n",
    "    return math.log(len(ngramDocDict) / (n_containing(ngram, ngramDocDict)))\n",
    "\n",
    "def tfidf(ngram, doc_id, ngram_count, ngramDocDict):\n",
    "    return tf(ngram, doc_id, ngram_count, ngramDocDict) * idf(ngram, ngramDocDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_tfidf_doc(ngramDocDict):\n",
    "    \n",
    "    ngram_statement_tfidf = {}\n",
    "    for s_id in ngramDocDict:\n",
    "        ngram_statement_tfidf[s_id] = {}\n",
    "\n",
    "        for ngram in ngramDocDict[s_id]:\n",
    "            ngram_statement_tfidf[s_id][ngram] = tfidf(ngram, s_id, statement_word_count, ngramDocDict)\n",
    "            \n",
    "    return ngram_statement_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_tfidf = cal_tfidf_doc(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing top 10 TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop\n",
    "\n",
    "heap = []\n",
    "\n",
    "for s_id in statement_tfidf:\n",
    "    for word in statement_tfidf[s_id]:\n",
    "        heappush(heap, (statement_tfidf[s_id][word], word))\n",
    "        \n",
    "        if len(heap) > 10:\n",
    "            heappop(heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(heap):\n",
    "    r = heappop(heap)\n",
    "    print(r[1], r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_notes = death_notes_lemma\n",
    "# death_notes = death_notes_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_url = \"https://tfhub.dev/google/elmo/2\"\n",
    "elmo_model = hub.Module(elmo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce logging output.\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#     elmo_embedding = session.run(elmo_model(doc_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/elmo_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(elmo_embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = glove_embedding\n",
    "# embedding = spacy_embedding\n",
    "# embedding = ft_embedding\n",
    "# embedding = use_embedding\n",
    "# embedding = elmo_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(embedding).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "maximum = 25\n",
    "\n",
    "_ = plt.plot()\n",
    "distortions = []\n",
    "K = range(2, maximum+1)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=2018)\n",
    "    _ = kmeans.fit(embedding)\n",
    "    _ = distortions.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow\n",
    "_ = plt.plot(K, distortions, 'bx-')\n",
    "_ = plt.xlabel('k')\n",
    "_ = plt.ylabel('Distortion')\n",
    "_ = plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', random_state=2018).fit(embedding)\n",
    "clusters = kmeans.predict(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get notes in each clsuter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_clusters = {}\n",
    "for c in set(clusters):\n",
    "    note_clusters[c] = []\n",
    "    for i, txt in enumerate(death_notes):\n",
    "        if clusters[i] == c:\n",
    "            note_clusters[c].append(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(embedding)\n",
    "tsne = TSNE(n_components=2, random_state=512)\n",
    "reduced = tsne.fit_transform(arr)       \n",
    "t = np.array(reduced).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "markers = [\"x\", \"v\", \"o\", \"s\", \"*\", \">\", \"<\", \"P\", \n",
    "           '1', '2', '3', '4', 'h', \"d\", \"|\", \"+\"]\n",
    "colors = ['darkorange', 'steelblue', 'limegreen',  'salmon', 'y',  'violet', 'c', 'tomato', \n",
    "          'rosybrown', 'brown', 'darkmagenta', 'pink', 'gold', \"orange\", \"skyblue\", \"seagreen\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "for x, y, c in zip(t[0], t[1], clusters):\n",
    "    _ = ax.scatter(x, y, c=colors[c], marker=markers[c])\n",
    "\n",
    "types = []\n",
    "for c in set(clusters):\n",
    "    types.append(Line2D([], [], color=colors[c], marker=markers[c], label=c))\n",
    "\n",
    "# for i, c in enumerate(clusters):\n",
    "#     if c == 5:\n",
    "#         _ = ax.annotate(doc_txt[i], (t[0][i], t[1][i]), fontsize=18)\n",
    "\n",
    "_ = plt.legend(handles=types, loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(clusters)):\n",
    "    if clusters[i] == 3:\n",
    "        print(death_notes_ori[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top K Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
