{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "!pip install scrapy\n",
    "\n",
    "!pip install fastText\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow_hub\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lucmeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "from heapq import heappush, heappop\n",
    "from scipy.sparse import csr_matrix\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import fastText\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.selector import Selector\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Show Python version\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Offenders Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffenderInfoWriterPipeline(object):\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('data/offender_info_results.json', 'w+')\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffenderInfoSpider(scrapy.Spider):\n",
    "    name = \"OffenderInfo\"\n",
    "    start_urls = [\n",
    "        'http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html'\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.OffenderInfoWriterPipeline': 1}\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        print('A response from %s just arrived!', response.url)\n",
    "        sel = Selector(response)\n",
    "\n",
    "        table = sel.xpath('//table[@class=\"tdcj_table indent\"]/tr')\n",
    "        for tr in table[1:]:\n",
    "            url_info = urljoin(response.url, str(tr.xpath('td[2]/a/@href').extract_first()))\n",
    "            url_stmt = urljoin(response.url, str(tr.xpath('td[3]/a/@href').extract_first()))\n",
    "        \n",
    "            yield {\n",
    "                'first_name': tr.xpath('td[5]/text()').extract_first(),\n",
    "                'last_name': tr.xpath('td[4]/text()').extract_first(),\n",
    "                'age': tr.xpath('td[7]/text()').extract_first(),\n",
    "                'date': tr.xpath('td[8]/text()').extract_first(),\n",
    "                'race': tr.xpath('td[9]/text()').extract_first(),\n",
    "                'country': tr.xpath('td[10]/text()').extract_first(),\n",
    "                'info_link': url_info,\n",
    "                'death_note_link': url_stmt\n",
    "            }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-27 15:52:44 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)\n",
      "2018-11-27 15:52:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2p  14 Aug 2018), cryptography 2.3.1, Platform Darwin-17.7.0-x86_64-i386-64bit\n",
      "2018-11-27 15:52:44 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x1a4718ca58>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A response from %s just arrived! http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess()\n",
    "process.crawl(OffenderInfoSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first_name',\n",
      " 'last_name',\n",
      " 'age',\n",
      " 'date',\n",
      " 'race',\n",
      " 'country',\n",
      " 'info_link',\n",
      " 'death_note_link']\n"
     ]
    }
   ],
   "source": [
    "file = open('data/offender_info_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "line = lines[0]\n",
    "obj = json.loads(line)\n",
    "pprint([key for key in obj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Death Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "objs = []\n",
    "\n",
    "file = open('data/offender_info_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "for line in lines:\n",
    "    objs.append(json.loads(line))\n",
    "    urls.append(objs[-1]['death_note_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeathNoteWriterPipeline(object):\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('data/death_note_results.json', 'w+')\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeathNoteSpider(scrapy.Spider):\n",
    "    name = \"DeathNote\"\n",
    "    start_urls = urls\n",
    "    \n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.DeathNoteWriterPipeline': 1}\n",
    "    }\n",
    "\n",
    "    def parse(self, response):     \n",
    "        sel = Selector(response)\n",
    "    \n",
    "        first = str(sel.xpath('//div[@id=\"content_right\"]/p[6]/text()').extract_first()).strip()\n",
    "        second = str(sel.xpath('//div[@id=\"content_right\"]/p[7]/text()').extract_first()).strip()\n",
    "        \n",
    "        death_note = ''\n",
    "        if first and first != 'Last Statement:' and first != 'None':\n",
    "            death_note += first\n",
    "        if second and second != 'None':\n",
    "            death_note += second\n",
    "            \n",
    "        url = response.url\n",
    "        obj = [o for o in objs if o['death_note_link'] == url][0]\n",
    "        obj['death_note'] = death_note\n",
    "    \n",
    "        yield obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-27 15:53:12 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)\n",
      "2018-11-27 15:53:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2p  14 Aug 2018), cryptography 2.3.1, Platform Darwin-17.7.0-x86_64-i386-64bit\n",
      "2018-11-27 15:53:12 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 30, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x1a458267b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "process.crawl(DeathNoteSpider())\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first_name',\n",
      " 'last_name',\n",
      " 'age',\n",
      " 'date',\n",
      " 'race',\n",
      " 'country',\n",
      " 'info_link',\n",
      " 'death_note_link',\n",
      " 'death_note']\n"
     ]
    }
   ],
   "source": [
    "file = open('data/death_note_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "line = lines[0]\n",
    "obj = json.loads(line)\n",
    "pprint([key for key in obj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/death_note_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "objs = []\n",
    "for line in lines:\n",
    "    objs.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Format Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(objs)):\n",
    "    note = objs[i]['death_note']\n",
    "    if len(note.strip()) > 0:\n",
    "        note = (note.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "        objs[i]['death_note'] = note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuations and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) - set('not')\n",
    "rm_punc = re.compile('[^a-zA-Z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs_clean = []\n",
    "death_notes_clean = []\n",
    "for i in range(len(objs)):\n",
    "    note = objs[i]['death_note']\n",
    "    note = rm_punc.sub(' ', note)\n",
    "    words = []\n",
    "    tokens = note.split()\n",
    "    for token in tokens:\n",
    "        if token and token not in stop_words and token.lower() not in stop_words \\\n",
    "        and token.lower() != 'none':\n",
    "            words.append(token)\n",
    "    \n",
    "    if words:\n",
    "        note = ' '.join(words)\n",
    "        death_notes_clean.append(note)\n",
    "        objs[i]['death_note'] = note\n",
    "        objs_clean.append(objs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/death_notes_clean.pickle', 'wb') as handle:\n",
    "    pickle.dump(objs_clean, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(death_notes_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs_lemma = []\n",
    "death_notes_lemma = []\n",
    "for i in range(len(objs_clean)):\n",
    "    obj = objs_clean[i]\n",
    "    \n",
    "    note = clean(obj['death_note'])\n",
    "    death_notes_lemma.append(note)\n",
    "    obj['death_note'] = note\n",
    "    objs_lemma.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(death_notes_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/death_notes_lemma.pickle', 'wb') as handle:\n",
    "    pickle.dump(objs_lemma, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per doc\n",
    "bag_of_words = {}\n",
    "statement_word_count = {}\n",
    "for s_id in range(len(death_notes_lemma)):\n",
    "    words_list = death_notes_lemma[s_id]['death_note'].split()\n",
    "    statement_word_count[s_id] = len(words_list)\n",
    "    bag_of_words[s_id] = {}\n",
    "    for word in words_list:\n",
    "        if word in bag_of_words[s_id]:\n",
    "            bag_of_words[s_id][word] = bag_of_words[s_id][word] +1\n",
    "        else:\n",
    "            bag_of_words[s_id][word] = 1\n",
    "\n",
    "for s_id in bag_of_words:\n",
    "    sorted_x = dict(sorted(bag_of_words[s_id].items(), key=lambda kv: kv[1], reverse=True))\n",
    "    bag_of_words[s_id] = sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all doc\n",
    "statementFreqDict = {}\n",
    "\n",
    "for s_id in bag_of_words:\n",
    "    for word in bag_of_words[s_id]:\n",
    "        if word in statementFreqDict:\n",
    "            statementFreqDict[word] = statementFreqDict[word] + bag_of_words[s_id][word]\n",
    "        else:\n",
    "            statementFreqDict[word] = bag_of_words[s_id][word]\n",
    "        \n",
    "sorted_x = dict(sorted(statementFreqDict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "statementFreqDict = sorted_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claculating TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(ngram, doc_id, ngram_count, ngramDocDict):\n",
    "    return ngramDocDict[doc_id][ngram] / ngram_count[doc_id]\n",
    "\n",
    "def n_containing(ngram, ngramDocDict):\n",
    "    return sum(1 for doc_id in ngramDocDict if ngram in ngramDocDict[doc_id])\n",
    "\n",
    "def idf(ngram, ngramDocDict):\n",
    "    return math.log(len(ngramDocDict) / (n_containing(ngram, ngramDocDict)))\n",
    "\n",
    "def tfidf(ngram, doc_id, ngram_count, ngramDocDict):\n",
    "    return tf(ngram, doc_id, ngram_count, ngramDocDict) * idf(ngram, ngramDocDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_tfidf_doc(ngramDocDict):\n",
    "    ngram_statement_tfidf = {}\n",
    "    for s_id in ngramDocDict:\n",
    "        ngram_statement_tfidf[s_id] = {}\n",
    "\n",
    "        for ngram in ngramDocDict[s_id]:\n",
    "            ngram_statement_tfidf[s_id][ngram] = tfidf(ngram, s_id, statement_word_count, ngramDocDict)\n",
    "            \n",
    "    return ngram_statement_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_encoding(ls):\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(ls)\n",
    "    tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "    X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "    return X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_tfidf = cal_tfidf_doc(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Top 10 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heap = []\n",
    "for s_id in statement_tfidf:\n",
    "    for word in statement_tfidf[s_id]:\n",
    "        heappush(heap, (statement_tfidf[s_id][word], word))\n",
    "        \n",
    "        if len(heap) > 10:\n",
    "            heappop(heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(heap):\n",
    "    r = heappop(heap)\n",
    "    print(r[1], r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_notes = death_notes_lemma\n",
    "#death_notes = death_notes_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_words = []\n",
    "for s_id in range(len(death_notes)):\n",
    "    last_words.append(death_notes_lemma[s_id]['death_note'])\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(last_words)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf = csr_matrix.todense(X_train_tf)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf = csr_matrix.todense(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google W2V\n",
    "* trained on Google News corpus\n",
    "* model download link: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Google News model\n",
    "filename = 'embedding/GoogleNews-vectors-negative300.bin'\n",
    "google_model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "vocab = google_model.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_embedding_avg = [] \n",
    "google_embedding_tfidf = []\n",
    "word_vect_list = [] # list storing only vectors of the words\n",
    "word_not_vocab = [] # list storing words which are nt in the Google news vocab\n",
    "\n",
    "for s_id in range(len(death_notes_lemma)): \n",
    "    words_list = death_notes_lemma[s_id]['death_note'].split()\n",
    "    vec = 0\n",
    "    count = 0\n",
    "    vec_tfidf = 0\n",
    "    tfidf_total = 0\n",
    "    for word in words_list:\n",
    "        if word in vocab:\n",
    "            vec = vec + google_model[word]\n",
    "            vec_tfidf += google_model[word] * statement_tfidf[s_id][word]\n",
    "            tfidf_total += statement_tfidf[s_id][word]\n",
    "            count += 1\n",
    "        else:\n",
    "            word_not_vocab.append(word)\n",
    "    google_embedding_avg.append(vec/count)\n",
    "    google_embedding_tfidf.append(vec_tfidf/tfidf_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/google_avg_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(google_embedding_avg, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/google_weighted_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(google_embedding_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy W2V\n",
    "* trained on written web text (blogs, news, comments)\n",
    "* model link: https://spacy.io/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_md = spacy.load('en_core_web_md')\n",
    "spacy_embedding = []\n",
    "for s_id in range(len(death_notes_lemma)): \n",
    "    doc = nlp_md(death_notes_lemma[s_id]['death_note'])\n",
    "    spacy_embedding.append(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/spacy_avg_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(spacy_embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted average\n",
    "spacy_embedding_tfidf = []\n",
    "for s_id in range(len(death_notes_lemma)): \n",
    "    words_list = death_notes_lemma[s_id]['death_note'].split()\n",
    "    vec_tfidf_spacy = 0\n",
    "    tfidf_total = 0\n",
    "    for word in words_list:\n",
    "        doc = nlp_md(word)\n",
    "        vec_tfidf_spacy += doc.vector * statement_tfidf[s_id][word]\n",
    "        tfidf_total += statement_tfidf[s_id][word]\n",
    "        spacy_embedding_tfidf.append(vec_tfidf_spacy/tfidf_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/spacy_weighted_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(spacy_embedding_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embedding\n",
    "* trained on Wikipedia 2014 + English Gigaword 5th\n",
    "* model download link: http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Stanford GloVe model\n",
    "filename = 'embedding/glove.6B.300d.txt.word2vec'\n",
    "glove_model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "glove_vocab = glove_model.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_avg = [] #list to store words and their corresponding vectors\n",
    "glove_embedding_tfidf = []\n",
    "word_not_vocab_glove = [] # list storing words which are nt in the Google news vocab\n",
    "\n",
    "for s_id in range(len(death_notes_lemma)): \n",
    "    words_list = death_notes_lemma[s_id]['death_note'].split()\n",
    "    vec = 0\n",
    "    count = 0\n",
    "    vec_tfidf = 0\n",
    "    tfidf_total = 0\n",
    "    for word in words_list:\n",
    "        if word in glove_vocab:\n",
    "            vec = vec + glove_model[word]\n",
    "            vec_tfidf += glove_model[word] * statement_tfidf[s_id][word]\n",
    "            tfidf_total += statement_tfidf[s_id][word]\n",
    "            count += 1\n",
    "        else:\n",
    "            word_not_vocab_glove.append(word)\n",
    "    glove_embedding_avg.append(vec/count)\n",
    "    glove_embedding_tfidf.append(vec_tfidf/tfidf_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/glove_avg_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(glove_embedding_avg, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/glove_weighted_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(glove_embedding_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText\n",
    "* trained on Common Crawl (600B tokens)\n",
    "* model download link: https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "fasttext_model = fastText.load_model('embedding/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embedding\n",
    "fasttext_embedding = []\n",
    "for s_id in range(len(death_notes_lemma)):  \n",
    "    fasttext_embedding.append(fasttext_model.get_sentence_vector(death_notes_lemma[s_id]['death_note']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/fastText_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(fasttext_embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_embedding_avg = [] #dictionary to store words and their corresponding vectors\n",
    "ft_embedding_tfidf = {}\n",
    "\n",
    "for s_id in range(len(death_notes_lemma)):\n",
    "    vec = 0\n",
    "    c = 0\n",
    "    vec_tfidf = 0\n",
    "    tfidf_total = 0\n",
    "    for word in word_tokenize(death_notes_lemma[s_id]['death_note']):\n",
    "        vec = vec + fasttext_model.get_word_vector(word)\n",
    "        vec_tfidf = vec_tfidf + fasttext_model.get_word_vector(word) * statement_tfidf[s_id][word]\n",
    "        tfidf_total += statement_tfidf[s_id][word]\n",
    "        c = c + 1\n",
    "    ft_embedding_avg.append(vec/c)\n",
    "    ft_embedding_tfidf.append(vec_tfidf/tfidf_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/ft_avg_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(ft_embedding_avg, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/ft_weighted_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(ft_embedding_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder\n",
    "* trained on Stanford Natural Language Inference (SNLI) corpus (570k human-written English sentence pairs)\n",
    "* model link: https://tfhub.dev/google/universal-sentence-encoder/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "use_model = hub.Module(use_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    use_embedding = session.run(use_model(death_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/use_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(use_embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elmo\n",
    "* trained on 1 Billion Word Benchmark\n",
    "* model link: https://tfhub.dev/google/elmo/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_url = \"https://tfhub.dev/google/elmo/2\"\n",
    "elmo_model = hub.Module(elmo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    elmo_embedding = session.run(elmo_model(death_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/elmo_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(elmo_embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = X_train_tfidf\n",
    "#embedding = google_embedding_avg\n",
    "# embedding = google_embedding_tfidf\n",
    "# embedding = glove_embedding_avg\n",
    "# embedding = glove_embedding_tfidf\n",
    "# embedding = spacy_embedding\n",
    "# embedding = spacy_embedding_tfidf\n",
    "# embedding = ft_embedding\n",
    "# embedding = ft_embedding_avg\n",
    "# embedding = ft_embedding_tfidf\n",
    "# embedding = use_embedding\n",
    "# embedding = elmo_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(embedding).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "maximum = 25\n",
    "\n",
    "_ = plt.plot()\n",
    "distortions = []\n",
    "K = range(2, maximum+1)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=2018)\n",
    "    _ = kmeans.fit(embedding)\n",
    "    _ = distortions.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow\n",
    "_ = plt.plot(K, distortions, 'bx-')\n",
    "_ = plt.xlabel('k')\n",
    "_ = plt.ylabel('Distortion')\n",
    "_ = plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', random_state=2018).fit(embedding)\n",
    "clusters = kmeans.predict(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get notes in each clsuter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_clusters = {}\n",
    "for c in set(clusters):\n",
    "    note_clusters[c] = []\n",
    "    for i, txt in enumerate(death_notes):\n",
    "        if clusters[i] == c:\n",
    "            note_clusters[c].append(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(embedding)\n",
    "tsne = TSNE(n_components=2, random_state=2018)\n",
    "reduced = tsne.fit_transform(arr)       \n",
    "t = np.array(reduced).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = [\"x\", \"v\", \"o\", \"s\", \"*\", \">\", \"<\", \"P\", \n",
    "           '1', '2', '3', '4', 'h', \"d\", \"|\", \"+\"]\n",
    "colors = ['darkorange', 'steelblue', 'limegreen',  'salmon', 'y',  'violet', 'c', 'tomato', \n",
    "          'rosybrown', 'brown', 'darkmagenta', 'pink', 'gold', \"orange\", \"skyblue\", \"seagreen\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "for x, y, c in zip(t[0], t[1], clusters):\n",
    "    _ = ax.scatter(x, y, c=colors[c], marker=markers[c])\n",
    "\n",
    "types = []\n",
    "for c in set(clusters):\n",
    "    types.append(Line2D([], [], color=colors[c], marker=markers[c], label=c))\n",
    "\n",
    "# for i, c in enumerate(clusters):\n",
    "#     if c == 5:\n",
    "#         _ = ax.annotate(doc_txt[i], (t[0][i], t[1][i]), fontsize=18)\n",
    "\n",
    "_ = plt.legend(handles=types, loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 words in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per doc within cluster\n",
    "bag_of_words = {}\n",
    "\n",
    "for c_id in range(len(note_clusters)):\n",
    "    bag_of_words[c_id] = {}\n",
    "    \n",
    "    for s_id in range(len(note_clusters[c_id])):\n",
    "        words_list = note_clusters[c_id][s_id]['death_note'].split()\n",
    "        bag_of_words[c_id][s_id] = {}\n",
    "        for word in words_list:\n",
    "            if word in bag_of_words[c_id][s_id]:\n",
    "                bag_of_words[c_id][s_id][word] = bag_of_words[c_id][s_id][word] +1\n",
    "            else:\n",
    "                bag_of_words[c_id][s_id][word] = 1\n",
    "\n",
    "for c_id in bag_of_words:\n",
    "    for s_id in bag_of_words[c_id]:\n",
    "        sorted_x = dict(sorted(bag_of_words[c_id][s_id].items(), key=lambda kv: kv[1], reverse=True))\n",
    "        bag_of_words[c_id][s_id] = sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all doc within each cluster\n",
    "\n",
    "statementFreqDict = {}\n",
    "for c_id in bag_of_words:\n",
    "    statementFreqDict[c_id] = {}\n",
    "    for s_id in bag_of_words[c_id]:\n",
    "        for word in bag_of_words[c_id][s_id]:\n",
    "            if word in statementFreqDict[c_id]:\n",
    "                statementFreqDict[c_id][word] = statementFreqDict[c_id][word] + bag_of_words[c_id][s_id][word]\n",
    "            else:\n",
    "                statementFreqDict[c_id][word] = bag_of_words[c_id][s_id][word]\n",
    "\n",
    "for c_id in bag_of_words:\n",
    "    sorted_x = dict(sorted(statementFreqDict[c_id].items(), key=lambda kv: kv[1], reverse=True))\n",
    "    statementFreqDict[c_id] = sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statementFreqDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2600\n",
    "n_topics = 5\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % (topic_idx+1), \", \".join([feature_names[i] + \"(\" + str(topic[i]) + \")\" for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words_f(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % (topic_idx+1))\n",
    "        for i in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "            print(feature_names[i] + \"\\t\", int(topic[i]*100))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.021s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(death_notes_lemma)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model with tf-idf features, n_features=2600...\n",
      "done in 0.055s.\n",
      "Topic #1:\n",
      "love\t 188\n",
      "know\t 76\n",
      "tell\t 70\n",
      "ya\t 65\n",
      "want\t 62\n",
      "strong\t 40\n",
      "everybody\t 38\n",
      "care\t 32\n",
      "say\t 31\n",
      "kill\t 28\n",
      "stay\t 27\n",
      "mom\t 25\n",
      "kid\t 23\n",
      "good\t 22\n",
      "family\t 20\n",
      "yes\t 20\n",
      "life\t 20\n",
      "man\t 18\n",
      "let\t 18\n",
      "appreciate\t 17\n",
      "\n",
      "Topic #2:\n",
      "sorry\t 165\n",
      "family\t 62\n",
      "hope\t 48\n",
      "pain\t 47\n",
      "cause\t 45\n",
      "say\t 39\n",
      "like\t 35\n",
      "bring\t 34\n",
      "wish\t 25\n",
      "hurt\t 25\n",
      "know\t 25\n",
      "victim\t 24\n",
      "change\t 21\n",
      "year\t 21\n",
      "apologize\t 20\n",
      "forgive\t 20\n",
      "closure\t 19\n",
      "mr\t 16\n",
      "happen\t 15\n",
      "truly\t 14\n",
      "\n",
      "Topic #3:\n",
      "statement\t 141\n",
      "decline\t 85\n",
      "offender\t 83\n",
      "make\t 54\n",
      "write\t 19\n",
      "final\t 9\n",
      "speak\t 7\n",
      "point\t 2\n",
      "innocent\t 2\n",
      "say\t 1\n",
      "hold\t 1\n",
      "bobby\t 0\n",
      "capital\t 0\n",
      "prepare\t 0\n",
      "warden\t 0\n",
      "talk\t 0\n",
      "portion\t 0\n",
      "omit\t 0\n",
      "girl\t 0\n",
      "gladly\t 0\n",
      "\n",
      "Topic #4:\n",
      "thank\t 171\n",
      "family\t 80\n",
      "like\t 68\n",
      "friend\t 50\n",
      "warden\t 48\n",
      "love\t 48\n",
      "support\t 43\n",
      "yes\t 39\n",
      "ready\t 24\n",
      "jesus\t 22\n",
      "sir\t 21\n",
      "help\t 19\n",
      "sister\t 17\n",
      "apologize\t 15\n",
      "strong\t 15\n",
      "hope\t 13\n",
      "victim\t 13\n",
      "year\t 13\n",
      "wife\t 12\n",
      "brother\t 11\n",
      "\n",
      "Topic #5:\n",
      "forgive\t 95\n",
      "god\t 88\n",
      "lord\t 73\n",
      "peace\t 65\n",
      "ask\t 47\n",
      "pray\t 44\n",
      "jesus\t 42\n",
      "heart\t 33\n",
      "christ\t 28\n",
      "hope\t 28\n",
      "forgiveness\t 27\n",
      "home\t 27\n",
      "praise\t 26\n",
      "bless\t 24\n",
      "sin\t 20\n",
      "come\t 19\n",
      "life\t 19\n",
      "father\t 18\n",
      "say\t 17\n",
      "ready\t 15\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features, n_features=%d...\" % n_features)\n",
    "t0 = time()\n",
    "nmf_model = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words_f(nmf_model, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
