{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "!pip install scrapy\n",
    "\n",
    "!pip install fastText\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow_hub\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lucmeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "from heapq import heappush, heappop\n",
    "from scipy.sparse import csr_matrix\n",
    "from pprint import pprint\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import fastText\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.selector import Selector\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Show Python version\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Offenders Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffenderInfoWriterPipeline(object):\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('data/offender_info_results.json', 'w+')\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffenderInfoSpider(scrapy.Spider):\n",
    "    name = \"OffenderInfo\"\n",
    "    start_urls = [\n",
    "        'http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html'\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.OffenderInfoWriterPipeline': 1}\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        print('A response from %s just arrived!', response.url)\n",
    "        sel = Selector(response)\n",
    "\n",
    "        table = sel.xpath('//table[@class=\"tdcj_table indent\"]/tr')\n",
    "        for tr in table[1:]:\n",
    "            url_info = urljoin(response.url, str(tr.xpath('td[2]/a/@href').extract_first()))\n",
    "            url_stmt = urljoin(response.url, str(tr.xpath('td[3]/a/@href').extract_first()))\n",
    "        \n",
    "            yield {\n",
    "                'first_name': tr.xpath('td[5]/text()').extract_first(),\n",
    "                'last_name': tr.xpath('td[4]/text()').extract_first(),\n",
    "                'age': tr.xpath('td[7]/text()').extract_first(),\n",
    "                'date': tr.xpath('td[8]/text()').extract_first(),\n",
    "                'race': tr.xpath('td[9]/text()').extract_first(),\n",
    "                'country': tr.xpath('td[10]/text()').extract_first(),\n",
    "                'info_link': url_info,\n",
    "                'death_note_link': url_stmt\n",
    "            }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process = CrawlerProcess()\n",
    "process.crawl(OffenderInfoSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/offender_info_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "line = lines[0]\n",
    "obj = json.loads(line)\n",
    "pprint([key for key in obj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Death Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "objs = []\n",
    "\n",
    "file = open('data/offender_info_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "for line in lines:\n",
    "    objs.append(json.loads(line))\n",
    "    urls.append(objs[-1]['death_note_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeathNoteWriterPipeline(object):\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('data/death_note_results.json', 'w+')\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeathNoteSpider(scrapy.Spider):\n",
    "    name = \"DeathNote\"\n",
    "    start_urls = urls\n",
    "    \n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.DeathNoteWriterPipeline': 1}\n",
    "    }\n",
    "\n",
    "    def parse(self, response):     \n",
    "        sel = Selector(response)\n",
    "    \n",
    "        first = str(sel.xpath('//div[@id=\"content_right\"]/p[6]/text()').extract_first()).strip()\n",
    "        second = str(sel.xpath('//div[@id=\"content_right\"]/p[7]/text()').extract_first()).strip()\n",
    "        \n",
    "        death_note = ''\n",
    "        if first and first != 'Last Statement:' and first != 'None':\n",
    "            death_note += first\n",
    "        if second and second != 'None':\n",
    "            death_note += second\n",
    "            \n",
    "        url = response.url\n",
    "        obj = [o for o in objs if o['death_note_link'] == url][0]\n",
    "        obj['death_note'] = death_note\n",
    "    \n",
    "        yield obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "process.crawl(DeathNoteSpider())\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/death_note_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "line = lines[0]\n",
    "obj = json.loads(line)\n",
    "pprint([key for key in obj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/death_note_results.json', 'r')\n",
    "lines = file.readlines()\n",
    "objs = []\n",
    "for line in lines:\n",
    "    objs.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Format Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(objs)):\n",
    "    note = objs[i]['death_note']\n",
    "    if len(note.strip()) > 0:\n",
    "        note = (note.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "        objs[i]['death_note'] = note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuations and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) - set('not')\n",
    "rm_punc = re.compile('[^a-zA-Z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs_clean = []\n",
    "death_notes_clean = []\n",
    "for i in range(len(objs)):\n",
    "    note = objs[i]['death_note']\n",
    "    note = rm_punc.sub(' ', note)\n",
    "    words = []\n",
    "    tokens = note.split()\n",
    "    for token in tokens:\n",
    "        if token and token not in stop_words and token.lower() not in stop_words \\\n",
    "        and token.lower() != 'none':\n",
    "            words.append(token)\n",
    "    \n",
    "    if words:\n",
    "        note = ' '.join(words)\n",
    "        death_notes_clean.append(note)\n",
    "        objs[i]['death_note'] = note\n",
    "        objs_clean.append(objs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/death_notes_clean.pickle', 'wb') as handle:\n",
    "    pickle.dump(objs_clean, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(death_notes_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs_lemma = []\n",
    "death_notes_lemma = []\n",
    "for i in range(len(objs_clean)):\n",
    "    obj = objs_clean[i]\n",
    "    \n",
    "    note = clean(obj['death_note'])\n",
    "    death_notes_lemma.append(note)\n",
    "    obj['death_note'] = note\n",
    "    objs_lemma.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(death_notes_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/death_notes_lemma.pickle', 'wb') as handle:\n",
    "    pickle.dump(objs_lemma, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per doc\n",
    "bag_of_words = {}\n",
    "statement_word_count = {}\n",
    "for s_id in range(len(death_notes_lemma)):\n",
    "    words_list = death_notes_lemma[s_id]['death_note'].split()\n",
    "    statement_word_count[s_id] = len(words_list)\n",
    "    bag_of_words[s_id] = {}\n",
    "    for word in words_list:\n",
    "        if word in bag_of_words[s_id]:\n",
    "            bag_of_words[s_id][word] = bag_of_words[s_id][word] +1\n",
    "        else:\n",
    "            bag_of_words[s_id][word] = 1\n",
    "\n",
    "for s_id in bag_of_words:\n",
    "    sorted_x = dict(sorted(bag_of_words[s_id].items(), key=lambda kv: kv[1], reverse=True))\n",
    "    bag_of_words[s_id] = sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all doc\n",
    "statementFreqDict = {}\n",
    "\n",
    "for s_id in bag_of_words:\n",
    "    for word in bag_of_words[s_id]:\n",
    "        if word in statementFreqDict:\n",
    "            statementFreqDict[word] = statementFreqDict[word] + bag_of_words[s_id][word]\n",
    "        else:\n",
    "            statementFreqDict[word] = bag_of_words[s_id][word]\n",
    "        \n",
    "sorted_x = dict(sorted(statementFreqDict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "statementFreqDict = sorted_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claculating TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(ngram, doc_id, ngram_count, ngramDocDict):\n",
    "    return ngramDocDict[doc_id][ngram] / ngram_count[doc_id]\n",
    "\n",
    "def n_containing(ngram, ngramDocDict):\n",
    "    return sum(1 for doc_id in ngramDocDict if ngram in ngramDocDict[doc_id])\n",
    "\n",
    "def idf(ngram, ngramDocDict):\n",
    "    return math.log(len(ngramDocDict) / (n_containing(ngram, ngramDocDict)))\n",
    "\n",
    "def tfidf(ngram, doc_id, ngram_count, ngramDocDict):\n",
    "    return tf(ngram, doc_id, ngram_count, ngramDocDict) * idf(ngram, ngramDocDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_tfidf_doc(ngramDocDict):\n",
    "    ngram_statement_tfidf = {}\n",
    "    for s_id in ngramDocDict:\n",
    "        ngram_statement_tfidf[s_id] = {}\n",
    "\n",
    "        for ngram in ngramDocDict[s_id]:\n",
    "            ngram_statement_tfidf[s_id][ngram] = tfidf(ngram, s_id, statement_word_count, ngramDocDict)\n",
    "            \n",
    "    return ngram_statement_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_encoding(ls):\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(ls)\n",
    "    tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "    X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "    return X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_tfidf = cal_tfidf_doc(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Top 10 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heap = []\n",
    "for s_id in statement_tfidf:\n",
    "    for word in statement_tfidf[s_id]:\n",
    "        heappush(heap, (statement_tfidf[s_id][word], word))\n",
    "        \n",
    "        if len(heap) > 10:\n",
    "            heappop(heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(heap):\n",
    "    r = heappop(heap)\n",
    "    print(r[1], r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_notes = death_notes_lemma\n",
    "#death_notes = death_notes_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_words = []\n",
    "for s_id in range(len(death_notes)):\n",
    "    last_words.append(death_notes_lemma[s_id]['death_note'])\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(last_words)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf = csr_matrix.todense(X_train_tf)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf = csr_matrix.todense(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google W2V\n",
    "* trained on Google News corpus\n",
    "* model download link: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Google News model\n",
    "filename = 'embedding/GoogleNews-vectors-negative300.bin'\n",
    "google_model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "vocab = google_model.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_embedding_avg = [] \n",
    "google_embedding_tfidf = []\n",
    "word_vect_list = [] # list storing only vectors of the words\n",
    "word_not_vocab = [] # list storing words which are nt in the Google news vocab\n",
    "\n",
    "for s_id in range(len(death_notes_lemma)): \n",
    "    words_list = death_notes_lemma[s_id]['death_note'].split()\n",
    "    vec = 0\n",
    "    count = 0\n",
    "    vec_tfidf = 0\n",
    "    tfidf_total = 0\n",
    "    for word in words_list:\n",
    "        if word in vocab:\n",
    "            vec = vec + google_model[word]\n",
    "            vec_tfidf += google_model[word] * statement_tfidf[s_id][word]\n",
    "            tfidf_total += statement_tfidf[s_id][word]\n",
    "            count += 1\n",
    "        else:\n",
    "            word_not_vocab.append(word)\n",
    "    google_embedding_avg.append(vec/count)\n",
    "    google_embedding_tfidf.append(vec_tfidf/tfidf_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/google_avg_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(google_embedding_avg, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/google_weighted_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(google_embedding_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy W2V\n",
    "* trained on written web text (blogs, news, comments)\n",
    "* model link: https://spacy.io/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_md = spacy.load('en_core_web_md')\n",
    "spacy_embedding = []\n",
    "for s_id in range(len(death_notes_lemma)): \n",
    "    doc = nlp_md(death_notes_lemma[s_id]['death_note'])\n",
    "    spacy_embedding.append(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/spacy_avg_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(spacy_embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted average\n",
    "spacy_embedding_tfidf = []\n",
    "for s_id in range(len(death_notes_lemma)): \n",
    "    words_list = death_notes_lemma[s_id]['death_note'].split()\n",
    "    vec_tfidf_spacy = 0\n",
    "    tfidf_total = 0\n",
    "    for word in words_list:\n",
    "        doc = nlp_md(word)\n",
    "        vec_tfidf_spacy += doc.vector * statement_tfidf[s_id][word]\n",
    "        tfidf_total += statement_tfidf[s_id][word]\n",
    "        spacy_embedding_tfidf.append(vec_tfidf_spacy/tfidf_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/spacy_weighted_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(spacy_embedding_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embedding\n",
    "* trained on Wikipedia 2014 + English Gigaword 5th\n",
    "* model download link: http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Stanford GloVe model\n",
    "filename = 'embedding/glove.6B.300d.txt.word2vec'\n",
    "glove_model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "glove_vocab = glove_model.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_avg = [] #list to store words and their corresponding vectors\n",
    "glove_embedding_tfidf = []\n",
    "word_not_vocab_glove = [] # list storing words which are nt in the Google news vocab\n",
    "\n",
    "for s_id in range(len(death_notes_lemma)): \n",
    "    words_list = death_notes_lemma[s_id]['death_note'].split()\n",
    "    vec = 0\n",
    "    count = 0\n",
    "    vec_tfidf = 0\n",
    "    tfidf_total = 0\n",
    "    for word in words_list:\n",
    "        if word in glove_vocab:\n",
    "            vec = vec + glove_model[word]\n",
    "            vec_tfidf += glove_model[word] * statement_tfidf[s_id][word]\n",
    "            tfidf_total += statement_tfidf[s_id][word]\n",
    "            count += 1\n",
    "        else:\n",
    "            word_not_vocab_glove.append(word)\n",
    "    glove_embedding_avg.append(vec/count)\n",
    "    glove_embedding_tfidf.append(vec_tfidf/tfidf_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/glove_avg_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(glove_embedding_avg, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/glove_weighted_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(glove_embedding_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText\n",
    "* trained on Common Crawl (600B tokens)\n",
    "* model download link: https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "fasttext_model = fastText.load_model('embedding/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embedding\n",
    "fasttext_embedding = []\n",
    "for s_id in range(len(death_notes_lemma)):  \n",
    "    fasttext_embedding.append(fasttext_model.get_sentence_vector(death_notes_lemma[s_id]['death_note']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/fastText_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(fasttext_embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_embedding_avg = [] #dictionary to store words and their corresponding vectors\n",
    "ft_embedding_tfidf = {}\n",
    "\n",
    "for s_id in range(len(death_notes_lemma)):\n",
    "    vec = 0\n",
    "    c = 0\n",
    "    vec_tfidf = 0\n",
    "    tfidf_total = 0\n",
    "    for word in word_tokenize(death_notes_lemma[s_id]['death_note']):\n",
    "        vec = vec + fasttext_model.get_word_vector(word)\n",
    "        vec_tfidf = vec_tfidf + fasttext_model.get_word_vector(word) * statement_tfidf[s_id][word]\n",
    "        tfidf_total += statement_tfidf[s_id][word]\n",
    "        c = c + 1\n",
    "    ft_embedding_avg.append(vec/c)\n",
    "    ft_embedding_tfidf.append(vec_tfidf/tfidf_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/ft_avg_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(ft_embedding_avg, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/ft_weighted_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(ft_embedding_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder\n",
    "* trained on Stanford Natural Language Inference (SNLI) corpus (570k human-written English sentence pairs)\n",
    "* model link: https://tfhub.dev/google/universal-sentence-encoder/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "use_model = hub.Module(use_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    use_embedding = session.run(use_model(death_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/use_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(use_embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elmo\n",
    "* trained on 1 Billion Word Benchmark\n",
    "* model link: https://tfhub.dev/google/elmo/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_url = \"https://tfhub.dev/google/elmo/2\"\n",
    "elmo_model = hub.Module(elmo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    elmo_embedding = session.run(elmo_model(death_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = \"data/elmo_embedding.pickle\"\n",
    "with open(embed_file, 'wb') as handle:\n",
    "    pickle.dump(elmo_embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = X_train_tfidf\n",
    "#embedding = google_embedding_avg\n",
    "# embedding = google_embedding_tfidf\n",
    "# embedding = glove_embedding_avg\n",
    "# embedding = glove_embedding_tfidf\n",
    "# embedding = spacy_embedding\n",
    "# embedding = spacy_embedding_tfidf\n",
    "# embedding = ft_embedding\n",
    "# embedding = ft_embedding_avg\n",
    "# embedding = ft_embedding_tfidf\n",
    "# embedding = use_embedding\n",
    "# embedding = elmo_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(embedding).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "maximum = 25\n",
    "\n",
    "_ = plt.plot()\n",
    "distortions = []\n",
    "K = range(2, maximum+1)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=2018)\n",
    "    _ = kmeans.fit(embedding)\n",
    "    _ = distortions.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow\n",
    "_ = plt.plot(K, distortions, 'bx-')\n",
    "_ = plt.xlabel('k')\n",
    "_ = plt.ylabel('Distortion')\n",
    "_ = plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', random_state=2018).fit(embedding)\n",
    "clusters = kmeans.predict(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get notes in each clsuter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_clusters = {}\n",
    "for c in set(clusters):\n",
    "    note_clusters[c] = []\n",
    "    for i, txt in enumerate(death_notes):\n",
    "        if clusters[i] == c:\n",
    "            note_clusters[c].append(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(embedding)\n",
    "tsne = TSNE(n_components=2, random_state=2018)\n",
    "reduced = tsne.fit_transform(arr)       \n",
    "t = np.array(reduced).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "markers = [\"x\", \"v\", \"o\", \"s\", \"*\", \">\", \"<\", \"P\", \n",
    "           '1', '2', '3', '4', 'h', \"d\", \"|\", \"+\"]\n",
    "colors = ['darkorange', 'steelblue', 'limegreen',  'salmon', 'y',  'violet', 'c', 'tomato', \n",
    "          'rosybrown', 'brown', 'darkmagenta', 'pink', 'gold', \"orange\", \"skyblue\", \"seagreen\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "for x, y, c in zip(t[0], t[1], clusters):\n",
    "    _ = ax.scatter(x, y, c=colors[c], marker=markers[c])\n",
    "\n",
    "types = []\n",
    "for c in set(clusters):\n",
    "    types.append(Line2D([], [], color=colors[c], marker=markers[c], label=c))\n",
    "\n",
    "# for i, c in enumerate(clusters):\n",
    "#     if c == 5:\n",
    "#         _ = ax.annotate(doc_txt[i], (t[0][i], t[1][i]), fontsize=18)\n",
    "\n",
    "_ = plt.legend(handles=types, loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 words in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per doc within cluster\n",
    "bag_of_words = {}\n",
    "\n",
    "for c_id in range(len(note_clusters)):\n",
    "    bag_of_words[c_id] = {}\n",
    "    \n",
    "    for s_id in range(len(note_clusters[c_id])):\n",
    "        words_list = note_clusters[c_id][s_id]['death_note'].split()\n",
    "        bag_of_words[c_id][s_id] = {}\n",
    "        for word in words_list:\n",
    "            if word in bag_of_words[c_id][s_id]:\n",
    "                bag_of_words[c_id][s_id][word] = bag_of_words[c_id][s_id][word] +1\n",
    "            else:\n",
    "                bag_of_words[c_id][s_id][word] = 1\n",
    "\n",
    "for c_id in bag_of_words:\n",
    "    for s_id in bag_of_words[c_id]:\n",
    "        sorted_x = dict(sorted(bag_of_words[c_id][s_id].items(), key=lambda kv: kv[1], reverse=True))\n",
    "        bag_of_words[c_id][s_id] = sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all doc within each cluster\n",
    "\n",
    "statementFreqDict = {}\n",
    "for c_id in bag_of_words:\n",
    "    statementFreqDict[c_id] = {}\n",
    "    for s_id in bag_of_words[c_id]:\n",
    "        for word in bag_of_words[c_id][s_id]:\n",
    "            if word in statementFreqDict[c_id]:\n",
    "                statementFreqDict[c_id][word] = statementFreqDict[c_id][word] + bag_of_words[c_id][s_id][word]\n",
    "            else:\n",
    "                statementFreqDict[c_id][word] = bag_of_words[c_id][s_id][word]\n",
    "\n",
    "for c_id in bag_of_words:\n",
    "    sorted_x = dict(sorted(statementFreqDict[c_id].items(), key=lambda kv: kv[1], reverse=True))\n",
    "    statementFreqDict[c_id] = sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statementFreqDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
